version: '3.8'

services:
  nifi:
    image: apache/nifi:latest
    container_name: nifi
    ports:
      - "8080:8080"
    volumes:
      - ./raw_data_medium-utv_sorted.csv:/opt/nifi/nifi-current/raw_data_medium-utv_sorted.csv
      - ./core-site.xml:/opt/nifi/nifi-current/conf/core-site.xml
      - ./hdfs-site.xml:/opt/nifi/nifi-current/conf/hdfs-site.xml
    environment:
      - NIFI_WEB_HTTP_PORT=8080
    networks:
      - hadoop-net
    
  namenode:
    image: apache/hadoop:3
    hostname: namenode
    command: >
      /bin/bash -c "
        hdfs namenode &
        while ! nc -z localhost 9870; do   
          sleep 1 
        done 
        hdfs dfs -mkdir -p /nifi &&
        hdfs dfs -chown -R nifi:hadoop /nifi &&
        hdfs dfs -chmod -R 755 /nifi &&
        hdfs dfs -mkdir -p /stats &&
        hdfs dfs -chown -R nifi:hadoop /stats &&
        hdfs dfs -chmod -R 755 /stats &&
        tail -f /dev/null
      "
    ports:
      - 9870:9870
    env_file:
      - ./config
    environment:
        ENSURE_NAMENODE_DIR: "/tmp/hadoop-root/dfs/name"
    networks:
      - hadoop-net
    volumes:
      - ./hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml

  datanode1:
    image: apache/hadoop:3
    container_name: datanode1
    hostname: datanode1
    command: ["hdfs", "datanode"]
    env_file:
      - ./config   
    networks:
      - hadoop-net
    volumes:
      - ./hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml

  datanode2:
    image: apache/hadoop:3
    container_name: datanode2
    hostname: datanode2
    command: ["hdfs", "datanode"]
    env_file:
      - ./config   
    networks:
      - hadoop-net
    volumes:
      - ./hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
   

  resourcemanager:
    image: apache/hadoop:3
    hostname: resourcemanager
    command: ["yarn", "resourcemanager"]
    ports:
        - 8088:8088
    env_file:
      - ./config
    volumes:
      - ./test.sh:/opt/test.sh
    networks:
      - hadoop-net


  nodemanager:
    image: apache/hadoop:3
    command: ["yarn", "nodemanager"]
    env_file:
      - ./config
    networks:
      - hadoop-net
    
  spark-master:
    #image: apache/spark:latest
    image: lucamjdimarco/sabd
    container_name: spark-master
    hostname: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_PORT=7077
      #- SPARK_MASTER_WEBUI_PORT=8080
      - SPARK_UI_PORT=4040
    ports:
      #- "8080:8080"
      - "7077:7077"
      - "4040:4040"
      - "4041:4041"
    networks:
      - hadoop-net
    command: >
      /bin/bash -c "
        /opt/spark/sbin/start-master.sh --webui-port 4040 &&
        tail -f /dev/null
      "
    volumes:
      - ./core-site.xml:/opt/spark/conf/core-site.xml
      - ./hdfs-site.xml:/opt/spark/conf/hdfs-site.xml

  # spark-worker:
  #   #image: apache/spark:latest
  #   image: lucamjdimarco/sabd
  #   container_name: spark-worker
  #   hostname: spark-worker
  #   environment:
  #     - SPARK_MODE=worker
  #     - SPARK_MASTER_URL=spark://spark-master:7077
  #     - SPARK_WORKER_MEMORY=1G
  #     - SPARK_WORKER_CORES=1
  #     - SPARK_WORKER_PORT=8082
  #   ports:
  #     - "8082:8082"
  #   networks:
  #     - hadoop-net
  #   command: >
  #     /bin/bash -c "
  #       /opt/spark/sbin/start-worker.sh spark://spark-master:7077 &&
  #       tail -f /dev/null
  #     "
  #   volumes:
  #     - ./core-site.xml:/opt/spark/conf/core-site.xml
  #     - ./hdfs-site.xml:/opt/spark/conf/hdfs-site.xml

  spark-worker-1:
    image: lucamjdimarco/sabd
    container_name: spark-worker-1
    hostname: spark-worker-1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=2
      #- SPARK_WORKER_PORT=8082
    ports:
      - "8082:8082"
      - "4042:4040"
    networks:
      - hadoop-net
    command: >
      /bin/bash -c "
        /opt/spark/sbin/start-worker.sh spark://spark-master:7077 &&
        tail -f /dev/null
      "
    volumes:
      - ./core-site.xml:/opt/spark/conf/core-site.xml
      - ./hdfs-site.xml:/opt/spark/conf/hdfs-site.xml
  
  spark-worker-2:
    image: lucamjdimarco/sabd
    container_name: spark-worker-2
    hostname: spark-worker-2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=2
      #- SPARK_WORKER_PORT=8083
    ports:
      - "8083:8083"
      - "4043:4040"
    networks:
      - hadoop-net
    command: >
      /bin/bash -c "
        /opt/spark/sbin/start-worker.sh spark://spark-master:7077 &&
        tail -f /dev/null
      "
    volumes:
      - ./core-site.xml:/opt/spark/conf/core-site.xml
      - ./hdfs-site.xml:/opt/spark/conf/hdfs-site.xml

  spark-worker-3:
    image: lucamjdimarco/sabd
    container_name: spark-worker-3
    hostname: spark-worker-3
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=2
      #- SPARK_WORKER_PORT=8083
    ports:
      - "8084:8084"
      - "4044:4040"
    networks:
      - hadoop-net
    command: >
      /bin/bash -c "
        /opt/spark/sbin/start-worker.sh spark://spark-master:7077 &&
        tail -f /dev/null
      "
    volumes:
      - ./core-site.xml:/opt/spark/conf/core-site.xml
      - ./hdfs-site.xml:/opt/spark/conf/hdfs-site.xml
  
  redis:
    image: redis:latest
    container_name: redis
    ports:
      - "6379:6379"
    networks:
      - hadoop-net

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
    networks:
      - hadoop-net
    depends_on:
      - redis
    volumes:
      - grafana-storage:/var/lib/grafana
  
networks:
  hadoop-net:
    driver: bridge

volumes:
  grafana-storage:
